{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sesion4_Regresion_Lasso_Ridge_JUPYTER.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWi-0Oj5uSF4"
      },
      "source": [
        "# **Presentación del caso**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuNsv4aMuYxk"
      },
      "source": [
        "Predecir o estimar el precio de una vivienda puede ser de gran ayuda a la hora  de tomar decisiones importantes tales como la adquisición de casa propia . A continuación se presenta un dataset compuesto por **25, 660 registros** para **Argentina y Colombia** adjunto a las siguientes **10 variables**:\n",
        "\n",
        "    1.   pais : \"Argentina\", \"Colombia\"\n",
        "    2.   provincia_departamento: Provincia o departamento (no ambas)  donde se ubica el departamento\n",
        "    3.   ciudad: Ciudad donde se ubica el departamento\n",
        "    4.   property_type: \"Departamento\" (siempre en Argentina), \"Apartamento\" (siempre en Colombia)\n",
        "    5.   operation_type: \"Venta\"\n",
        "    6.   rooms: cantidad de espacios en general dentro del apartamento\n",
        "    7.   bedrooms: cantidad de cuartos donde dormir dentro del apartamento\n",
        "    8.   bathrooms: cantidad de baños dentro del apartamento\n",
        "    9.   surface_total: área total en metros cuadrados del departamento\n",
        "    10.  currency: \"USD\" (dólar americano)\n",
        "\n",
        "![Image of Yaktocat](https://www.datasource.ai/uploads/7c2c64c37b855715637538ef4f19a46d.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVXIYLgptpI7"
      },
      "source": [
        "# **Lectura de los datos**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_b6A4dFtWtf"
      },
      "source": [
        "\"CELDA N°1\"\n",
        "#Importamos los datos de train y test considerando separador, índice y tipos de datos\n",
        "import pandas as pd\n",
        "data=pd.read_csv(\"https://raw.githubusercontent.com/HackSpacePeru/Datasets_intro_Data_Science/master/precios_argentina_colombia.csv\",sep=\";\", index_col='Id')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JL_aKONP-7y6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e0375f1-9014-40eb-fd49-455958b6d074"
      },
      "source": [
        "\"CELDA N°2\"\n",
        "#Comprobamos que la train tenga 25,660 filas y tanto train como test tengan 10 variables predictoras\n",
        "data.shape"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25660, 11)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI89NEH1SmSi"
      },
      "source": [
        "# **Eliminando variables no relevantes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrUl-Z6ESLPr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e500e2a9-0ce7-48ec-ddca-27c4b28d8233"
      },
      "source": [
        "\"CELDA N°3\"\n",
        "#Comprobamos que algunas columnas tienen un único valor en train\n",
        "for col in data.columns:\n",
        "  if data[col].nunique()<3:\n",
        "    print('En la columna',col,'hay',data[col].nunique(),'valores distintos')\n",
        "  else:\n",
        "     pass #no realizar ninguna acción si es que el número de valores distintos es mayor que 2"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "En la columna pais hay 2 valores distintos\n",
            "En la columna property_type hay 2 valores distintos\n",
            "En la columna operation_type hay 1 valores distintos\n",
            "En la columna currency hay 1 valores distintos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc_Ov6eMTKT4"
      },
      "source": [
        "Las columnas **pais, property_type, operation_type, currency** tienen apenas 1 o 2 valores repetidos en toda la columna.\n",
        "\n",
        "Por eso eliminamos estas columnas que no aportan data significativa para predecir el precio del departamento, **excepto la columna pais** porque será necesaria después para traer data externa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEwdeQsmT5XY"
      },
      "source": [
        "\"CELDA N°4\"\n",
        "#Eliminamos las columnas mencionadas con el método drop. No olvidar incluir inplace = True\n",
        "data.drop(columns=['property_type', 'operation_type', 'currency'], inplace=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BeAJ_Q8uO4N"
      },
      "source": [
        "# **Preprocesamiento de los datos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9kO4LXHyvfu"
      },
      "source": [
        "## I. Verificación de datos perdidos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtyIde-5gh48"
      },
      "source": [
        "\"CELDA N°5\"\n",
        "#Verificamos que ninguna columna de train tenga vacíos\n",
        "for col in data.columns:\n",
        "  if data[col].isna().sum()>0:\n",
        "    print('En la columna',col,'hay',data[col].isna().sum(),'valores nulos')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Sje730ofQ-f"
      },
      "source": [
        "Si al ejecutar la celda anterior **no obtenemos resultado** se debe a que no se encontró ninguna columna que tenga datos perdidos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiHB3qp3Aaua"
      },
      "source": [
        "## II. Verificación de outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ml8FgqurOMLW"
      },
      "source": [
        "Con ayuda del método **skew** descrita por la distribución de cada variable numérica detectaremos la presencia de **outliers**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxWoYa93alf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "688baedf-f266-4553-da83-6addd40f0545"
      },
      "source": [
        "\"CELDA N°6\"\n",
        "#Previamente seleccionamos las variables numéricas y en cada una calculamos el skew para mostrarlo en un dataframe\n",
        "data_to_skew = data.select_dtypes(include = [\"number\"])\n",
        "\n",
        "skew = []\n",
        "for col in data_to_skew.columns:\n",
        "  skew.append(data_to_skew[col].skew())\n",
        "\n",
        "skewness=pd.DataFrame(index=data_to_skew.columns) #declaramos como índices a las columnas numéricas\n",
        "skewness[\"Skewness\"] = skew\n",
        "skewness.sort_values(by=[\"Skewness\"], ascending=False) #ordenamos de mayor a menor"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Skewness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>price</th>\n",
              "      <td>6.026587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>surface_total</th>\n",
              "      <td>2.081696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bathrooms</th>\n",
              "      <td>1.378065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rooms</th>\n",
              "      <td>1.222084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bedrooms</th>\n",
              "      <td>0.522811</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Skewness\n",
              "price          6.026587\n",
              "surface_total  2.081696\n",
              "bathrooms      1.378065\n",
              "rooms          1.222084\n",
              "bedrooms       0.522811"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-xaurRNf8xl"
      },
      "source": [
        "Para corregir el **alto skewness** de las columnas **price y surface_total** aplicaremos una **transformación logarítmica**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiRZjhP3gIvP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47d2efc1-821a-4e0d-ac7a-1f5239acdb9c"
      },
      "source": [
        "\"CELDA N°7\"\n",
        "#Usamos una función lambda para realizar una transformación logarítmica usando numpy sobre todas las filas de la columna price\n",
        "import numpy as np\n",
        "data[\"price\"] = data[\"price\"].map(lambda i: np.log(i) if i > 0 else 0) \n",
        "print('El skewness después de la transformación logarítmica es: ',data[\"price\"].skew())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "El skewness después de la transformación logarítmica es:  0.8850647322573656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fsz9078ykhLw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03eb6470-b70c-4ed9-890f-bc49ac5d4c0a"
      },
      "source": [
        "\"CELDA N°8\"\n",
        "#Usamos una función lambda para realizar una transformación logarítmica usando numpy sobre todas las filas de la columna surface_total\n",
        "data[\"surface_total\"] = data[\"surface_total\"].map(lambda i: np.log(i) if i > 0 else 0) \n",
        "print('El skewness después de la transformación logarítmica es: ',data[\"surface_total\"].kurt())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "El skewness después de la transformación logarítmica es:  -0.0539266496430324\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1kX5fC9C-wC"
      },
      "source": [
        "## III. Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btJICdjJd91v"
      },
      "source": [
        "### *Añadimos data externa*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9LZWfsfXyD5"
      },
      "source": [
        "Juntando **país y provincia** obtenemos datos estadísticos sobre cada **provincia**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHY14QIIUSYm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b6b042b-cec7-42ae-98c1-6ea975e9487e"
      },
      "source": [
        "\"CELDA N°9\"\n",
        "#Leemos la tabla externa pais_provincia con promedio, medianas y percentiles según provincia de Argentina y Colombia\n",
        "pais_provincia=pd.read_csv(\"https://raw.githubusercontent.com/HackSpacePeru/Datasets_intro_Data_Science/master/pais_provincia.csv\",sep=\";\")\n",
        "pais_provincia.columns"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['pais_provincia', 'promedio_provincia', 'mediana_provincia',\n",
              "       'percentil10_provincia', 'percentil25_provincia',\n",
              "       'percentil75_provincia', 'percentil90_provincia'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfmA7UYfX-If"
      },
      "source": [
        "Añadimos **nuevas columnas** al final con ayuda del método **merge**. Previamente **concatenamos el país y provincia**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gl7M0sqSEQfZ"
      },
      "source": [
        "\"CELDA N°10\"\n",
        "#Para concatenar ambas columnas simplemente usamos el operador + con el guión bajo entre comillas\n",
        "data['pais_provincia'] = data['pais']+ \"_\" + data['provincia_departamento']\n",
        "\n",
        "#Para adjuntar los datos de la tabla externa usamos el método merge especificando left (LEFT JOIN)\n",
        "data = data.merge(pais_provincia, on='pais_provincia', how='left')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cQPp_DLa8Gp"
      },
      "source": [
        "Juntando **provincia y ciudad** obtenemos datos estadísticos sobre cada **ciudad**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oW55iFcbi3l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2210756-d6db-4827-d5a4-279d4e0dceb8"
      },
      "source": [
        "\"CELDA N°11\"\n",
        "#Leemos la tabla externa provincia_ciudad con promedio, medianas y percentiles según ciudad de Argentina y Colombia\n",
        "provincia_ciudad=pd.read_csv(\"https://raw.githubusercontent.com/HackSpacePeru/Datasets_intro_Data_Science/master/provincia_ciudad.csv\",sep=\";\")\n",
        "provincia_ciudad.columns"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['provincia_ciudad', 'area_ciudad', 'altura_ciudad', 'habitantes_ciudad',\n",
              "       'densidad_ciudad', 'promedio_ciudad', 'mediana_ciudad',\n",
              "       'percentil10_ciudad', 'percentil25_ciudad', 'percentil75_ciudad',\n",
              "       'percentil90_ciudad'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dJxdskk23kK"
      },
      "source": [
        "Para nuestro caso de **predicción de precios** nos enfocaremos **exclusivamente** en las variables que sean relevantes para este objetivo.\n",
        "\n",
        "Por ello vamos a **filtrar** esas columnas del dataset provincia_ciudad."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo96fVzm3SiP"
      },
      "source": [
        "\"CELDA N°12\"\n",
        "#Filtramos solo las columnas relativas al precio con el método loc\n",
        "provincia_ciudad = provincia_ciudad.loc[:,['provincia_ciudad',\n",
        "                                           'promedio_ciudad',\n",
        "                                           'mediana_ciudad',\n",
        "                                           'percentil10_ciudad',\n",
        "                                           'percentil25_ciudad',\n",
        "                                           'percentil75_ciudad',\n",
        "                                           'percentil90_ciudad']]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8kU7PBLbvRh"
      },
      "source": [
        "Añadimos **nuevas columnas** al final con ayuda del método **merge**. Previamente **concatenamos la provincia y ciudad**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyA9ZlfObsxq"
      },
      "source": [
        "\"CELDA N°13\"\n",
        "#Para concatenar ambas columnas simplemente usamos el operador + con el guión bajo entre comillas\n",
        "data['provincia_ciudad'] = data['provincia_departamento']+ \"_\" + data['ciudad']\n",
        "\n",
        "#Para adjuntar los datos de la tabla externa usamos el método merge especificando left (LEFT JOIN)\n",
        "data = data.merge(provincia_ciudad, on='provincia_ciudad', how='left')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_vH4DeuXz-M"
      },
      "source": [
        "## IV. Supuestos para Modelos de Regresión"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ltN-g_51SbU"
      },
      "source": [
        "### *Estandarización de las variables numéricas predictoras*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXu_iidRAYrd"
      },
      "source": [
        "Vamos a **estandarizar** usando StandardScaler modificando la distribución de los datos para asegurar la **normalidad** de los datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FB9AoGj1YKn"
      },
      "source": [
        "\"CELDA N°14\"\n",
        "#Seleccionamos solo las columnas numéricas de la data total\n",
        "data_to_standar = data.select_dtypes(include = [\"number\"])\n",
        "\n",
        "#Aplicamos la librería StandardScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "data_standar = StandardScaler().fit_transform(data_to_standar)\n",
        "data = pd.DataFrame(data=data_standar, columns=data_to_standar.columns)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "043zIXMJ8iER"
      },
      "source": [
        "### *Multicolinealidad*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ssZUl9g35i5"
      },
      "source": [
        "Vamos a verificar la **multicolinearidad** utilizando el método del **Variance Inflation Factor(VIF)**.\n",
        "\n",
        "Para mayor información consultar la siguiente [página](https://towardsdatascience.com/multi-collinearity-in-regression-fe7a2c1467ea)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlTfHcOTXzSA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "outputId": "2d3c2b1e-a135-445f-83d0-77157eea48d2"
      },
      "source": [
        "\"CELDA N°15\"\n",
        "#Aplicamos variance_inflation_factor sobre todas las columnas para obtener un dataframe\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "vif = pd.DataFrame()\n",
        "vif[\"features\"] = data.columns\n",
        "vif[\"vif_Factor\"] = [variance_inflation_factor(data.values, i) for i in range(data.shape[1])]\n",
        "vif"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>features</th>\n",
              "      <th>vif_Factor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rooms</td>\n",
              "      <td>4.824583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>bedrooms</td>\n",
              "      <td>6.871245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>bathrooms</td>\n",
              "      <td>3.298516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>surface_total</td>\n",
              "      <td>7.867226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>price</td>\n",
              "      <td>4.845436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>promedio_provincia</td>\n",
              "      <td>304.722250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>mediana_provincia</td>\n",
              "      <td>231.320329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>percentil10_provincia</td>\n",
              "      <td>114.810387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>percentil25_provincia</td>\n",
              "      <td>199.430680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>percentil75_provincia</td>\n",
              "      <td>336.628552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>percentil90_provincia</td>\n",
              "      <td>274.802734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>promedio_ciudad</td>\n",
              "      <td>1137.260505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>mediana_ciudad</td>\n",
              "      <td>217.258956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>percentil10_ciudad</td>\n",
              "      <td>59.271647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>percentil25_ciudad</td>\n",
              "      <td>1.017143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>percentil75_ciudad</td>\n",
              "      <td>219.300268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>percentil90_ciudad</td>\n",
              "      <td>101.878999</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 features   vif_Factor\n",
              "0                   rooms     4.824583\n",
              "1                bedrooms     6.871245\n",
              "2               bathrooms     3.298516\n",
              "3           surface_total     7.867226\n",
              "4                   price     4.845436\n",
              "5      promedio_provincia   304.722250\n",
              "6       mediana_provincia   231.320329\n",
              "7   percentil10_provincia   114.810387\n",
              "8   percentil25_provincia   199.430680\n",
              "9   percentil75_provincia   336.628552\n",
              "10  percentil90_provincia   274.802734\n",
              "11        promedio_ciudad  1137.260505\n",
              "12         mediana_ciudad   217.258956\n",
              "13     percentil10_ciudad    59.271647\n",
              "14     percentil25_ciudad     1.017143\n",
              "15     percentil75_ciudad   219.300268\n",
              "16     percentil90_ciudad   101.878999"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgjy66N64ULS"
      },
      "source": [
        "Se evidencia que los datos externos superan por mucho el límite permisible. Así como también algunas variables originales.\n",
        "\n",
        "Para corregirlo haremos uso del **PCA** (sin considerar el precio)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3hq4QHJ6LCZ"
      },
      "source": [
        "\"CELDA N°16\"\n",
        "#Separamos las variables predictoras del target\n",
        "X = data.drop('price',axis=1)\n",
        "y = data['price']"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPdxcEqY40Oa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "outputId": "51ff2155-cf35-4313-a20d-92c29c867f73"
      },
      "source": [
        "\"CELDA N°17\"\n",
        "#Aplicamos PCA sobre las variables predictoras y actualizamos X para obtener un nuevo dataframe de VIF\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=6)\n",
        "components=pca.fit_transform(X)\n",
        "X=pd.DataFrame(data=components,columns=['PCA1','PCA2','PCA3','PCA4','PCA5','PCA6'])\n",
        "\n",
        "vif = pd.DataFrame()\n",
        "vif[\"features\"] = X.columns\n",
        "vif[\"vif_value\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "vif"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>features</th>\n",
              "      <th>vif_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>PCA1</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>PCA2</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>PCA3</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>PCA4</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PCA5</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>PCA6</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  features  vif_value\n",
              "0     PCA1        1.0\n",
              "1     PCA2        1.0\n",
              "2     PCA3        1.0\n",
              "3     PCA4        1.0\n",
              "4     PCA5        1.0\n",
              "5     PCA6        1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyE5cfvQNLoS"
      },
      "source": [
        "Ahora ya hemos cumplido los **supuestos** para ejecutar un modelo de **regresión**:\n",
        "\n",
        "* Distribución normal (sin outliers) de las variables predictoras\n",
        "* Independencia entre las variables predictoras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22IFIJrOLyzA"
      },
      "source": [
        "##  V. Preparamos los datos para el modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNutY5DNOHNc"
      },
      "source": [
        "Finalmente, con ayuda de la librería **train_test_split** dividimos la data de **train**: 85% para entrenamiento y **15%** para validación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpNI_yiDOFyN"
      },
      "source": [
        "\"CELDA N°18\"\n",
        "#Con la librería train_test_split generamos los datos de entrenamiento y validación \n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_valid, y_train, y_valid= train_test_split(X,y,test_size = 0.15,random_state=1) "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCHxmr-rN6Dc"
      },
      "source": [
        "# **Modelamiento y Evaluación**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1kZ5OB6a5Ac"
      },
      "source": [
        "Importamos las librerías de **Scikit Learn** para realizar:\n",
        "1.   Regresión Lasso\n",
        "2.   Regresión Ridge\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_HlNWmc63XY"
      },
      "source": [
        "\"CELDA N°19\"\n",
        "#Obtenemos los modelos de la librería linear_models de sklearn\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "#Desactivamos los mensajes de advertencia para mejor visibilidad de los resultados de cada celda\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOCibPcMZuGX"
      },
      "source": [
        "\"CELDA N°20\"\n",
        "#Utilizaremos  el indicador denominado MSLE para obtener el error logarítmico promedio\n",
        "from sklearn.metrics import mean_squared_log_error"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feooWXz0SLVU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f185a813-ee71-4127-db3b-5fb450278646"
      },
      "source": [
        "\"CELDA N°21\"\n",
        "#Obtenemos el MSLE aplicando Lasso variando el valor de alpha\n",
        "for i in range(1,10):\n",
        "    lasso = Lasso(alpha=i/100).fit(X_train,y_train)\n",
        "    lasso_predictions=lasso.predict(X_valid)\n",
        "    print(\"Mi MSLE es: \", mean_squared_log_error(abs(y_valid),abs(lasso_predictions)), \"cuando alpha es: \", i/100)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mi MSLE es:  0.05450044535546668 cuando alpha es:  0.01\n",
            "Mi MSLE es:  0.054883735821263074 cuando alpha es:  0.02\n",
            "Mi MSLE es:  0.05567818466692163 cuando alpha es:  0.03\n",
            "Mi MSLE es:  0.056846166848497866 cuando alpha es:  0.04\n",
            "Mi MSLE es:  0.05842181813836395 cuando alpha es:  0.05\n",
            "Mi MSLE es:  0.06036424753898551 cuando alpha es:  0.06\n",
            "Mi MSLE es:  0.06271178101536869 cuando alpha es:  0.07\n",
            "Mi MSLE es:  0.06545731883217724 cuando alpha es:  0.08\n",
            "Mi MSLE es:  0.06860346677978614 cuando alpha es:  0.09\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h2INPZVGUr2"
      },
      "source": [
        "Para mayor detalle del porqué un valor acordado para **alpha = 0.01** sin basarnos en una base **teórica** acudir al [video](https://youtu.be/4b4MUYve_U8) **minuto 26** de la clase dictada por **AndreNg en Standford**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVfb6pzBK_3W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42931661-cbff-4ddd-fc91-05c2190b9514"
      },
      "source": [
        "\"CELDA N°22\"\n",
        "#Obtenemos el MSLE aplicando Lasso variando el valor de alpha\n",
        "for i in range(1,10):\n",
        "    ridge = Ridge(alpha=i/100).fit(X_train,y_train)\n",
        "    ridge_predictions=ridge.predict(X_valid)\n",
        "    print(\"Mi MSLE es: \", mean_squared_log_error(abs(y_valid),abs(ridge_predictions)), \"cuando alpha es: \", i/100)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mi MSLE es:  0.054490346410072596 cuando alpha es:  0.01\n",
            "Mi MSLE es:  0.05449033719651285 cuando alpha es:  0.02\n",
            "Mi MSLE es:  0.05449032798380175 cuando alpha es:  0.03\n",
            "Mi MSLE es:  0.05449031877193933 cuando alpha es:  0.04\n",
            "Mi MSLE es:  0.054490309560925536 cuando alpha es:  0.05\n",
            "Mi MSLE es:  0.0544903003507604 cuando alpha es:  0.06\n",
            "Mi MSLE es:  0.054490291141443874 cuando alpha es:  0.07\n",
            "Mi MSLE es:  0.054490281932975955 cuando alpha es:  0.08\n",
            "Mi MSLE es:  0.054490272725356656 cuando alpha es:  0.09\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgJLGcSOgwtZ"
      },
      "source": [
        "# **Cross Validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1tu9dDiRH-3"
      },
      "source": [
        "Para evaluar la **robustez** del modelo vamos a utilizar **Repeated KFoldes** para aumentar el número de iteraciones en la divisiión de **KFolds**.\n",
        "\n",
        "Puedes encontrar mayor información en este [enlace](https://machinelearningmastery.com/repeated-k-fold-cross-validation-with-python/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYFPLDBVN3wM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d1481d7-4fe5-4845-e648-25f4bf6ba100"
      },
      "source": [
        "\"CELDA N°23\"\n",
        "#Aplicamos las librerías correspondientes para ejecutar el Repeated KFoldes y Cross Validation\n",
        "\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "#Realizamos 5 particiones y 3 repeticiones\n",
        "cv = RepeatedKFold(n_splits=5, n_repeats=3)\n",
        "\n",
        "#Guardamos los resultados de aplicar el MSLE en la variable scores\n",
        "scores = -cross_val_score(Ridge(alpha=0.01), abs(X), abs(y), scoring='neg_mean_squared_log_error', cv=cv, n_jobs=-1)\n",
        "\n",
        "#Finalmente obtenemos el promedio y desviación estándar de los resultados\n",
        "print('Promedio y Desviación del Error: %.3f (%.3f)' % (mean(scores), std(scores)))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Promedio y Desviación del Error: 0.083 (0.002)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaAdTJjoiWvu"
      },
      "source": [
        "Lo **ideal** es que el error logarítmico promedio sea **mínimo**, por ello el Promedio debe ser **cercano a cero** (al igual que la desviación).\n",
        "\n",
        "De no cumplise alguna de estas condiciones es una alerta para mejorar el modelo para que sea más **robusto** y mejore su precisión para diferentes datos."
      ]
    }
  ]
}